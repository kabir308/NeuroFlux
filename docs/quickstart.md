# ğŸš€ DÃ©marrage Rapide NeuroFlux

## ğŸŒ En FranÃ§ais

Bienvenue dans le guide de dÃ©marrage rapide de NeuroFlux ! Suivez ces Ã©tapes simples pour commencer Ã  utiliser notre framework d'IA microscopique.

### ğŸ“‹ PrÃ©requis

- Python 3.10 ou supÃ©rieur
- Git
- Un environnement virtuel Python

### ğŸ“¦ Installation

1. **Clonez le Repository**
   ```bash
   git clone https://github.com/neuroflux/neuroflux.git
   cd neuroflux
   ```

2. **CrÃ©ez un Environnement Virtuel**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   venv\Scripts\activate     # Windows
   ```

3. **Installez les DÃ©pendances**
   ```bash
   pip install -r requirements.txt
   ```

### ğŸ§  AccÃ©der aux ModÃ¨les NeuroFlux PrÃ©-OptimisÃ©s

NeuroFlux fournit une suite de modÃ¨les IA prÃ©-optimisÃ©s pour le dÃ©ploiement sur des appareils pÃ©riphÃ©riques (edge).
Vous pouvez prÃ©parer ces modÃ¨les en utilisant l'utilitaire `NeuroFluxDataset` du module `huggingface`.

Assurez-vous d'abord d'avoir les bibliothÃ¨ques nÃ©cessaires pour la conversion et l'optimisation des modÃ¨les :
```bash
pip install -r huggingface/requirements.txt
# Vous pourriez Ã©galement avoir besoin d'installer des outils de conversion spÃ©cifiques comme onnx-tensorflow ou paddle2onnx
# pip install onnx onnx-tf paddle2onnx # Exemple
```

Ensuite, vous pouvez utiliser le script Python suivant pour gÃ©nÃ©rer les modÃ¨les optimisÃ©s :

```python
from huggingface.neuroflux import NeuroFluxDataset
import os

# CrÃ©ez une instance pour tous les modÃ¨les (ou un spÃ©cifique)
# Le 'name' dans NeuroFluxConfig dÃ©terminera quels modÃ¨les sont traitÃ©s.
# Pour cet exemple, supposons que vous ayez une configuration dans neuroflux.py
# qui traite un modÃ¨le comme 'mobilenet' ou 'efficientnet-lite'.

# Exemple : Traiter le type de modÃ¨le 'mobilenet'
print("PrÃ©paration du modÃ¨le NeuroFlux MobileNet...")
# Assurez-vous que "mobilenet" est un nom de configuration valide dans NeuroFluxDataset.BUILDER_CONFIGS
dataset_builder = NeuroFluxDataset(name="mobilenet")

# DÃ©finir un rÃ©pertoire de sortie pour le dataset
output_dir = "./neuroflux_optimized_models_fr" # RÃ©pertoire de sortie spÃ©cifique Ã  la langue
os.makedirs(output_dir, exist_ok=True)

# Ceci va tÃ©lÃ©charger, convertir et optimiser les modÃ¨les spÃ©cifiÃ©s par la configuration
# Les modÃ¨les TFLite rÃ©sultants et les mÃ©tadonnÃ©es seront dans 'output_dir'
dataset_builder.download_and_prepare(download_dir=output_dir)

print(f"Les modÃ¨les optimisÃ©s et les mÃ©tadonnÃ©es sont disponibles dans : {output_dir}")
print("Vous pouvez trouver les fichiers de modÃ¨le .tflite dans les sous-rÃ©pertoires respectifs des modÃ¨les.")
```

Cela inclut une variÃ©tÃ© de modÃ¨les (par exemple, pour la vision comme EfficientNet-Lite et NanoDet-Plus, des modÃ¨les NLP comme FastBERT, et plus encore). Ces modÃ¨les sont automatiquement convertis en TFLite et peuvent Ãªtre optimisÃ©s pour l'exÃ©cution GPU et la quantification hybride afin d'amÃ©liorer les performances sur les appareils mobiles/pÃ©riphÃ©riques.

### ğŸš€ ExÃ©cuter l'InfÃ©rence avec un ModÃ¨le TFLite

Une fois que vous avez gÃ©nÃ©rÃ© un modÃ¨le TFLite (par exemple, `mobilenet.tflite` dans le rÃ©pertoire `output_dir/mobilenet/`), vous pouvez exÃ©cuter une infÃ©rence en utilisant TensorFlow Lite Interpreter. Assurez-vous d'avoir `tensorflow` et `numpy` installÃ©s (`pip install tensorflow numpy`).

```python
import tensorflow as tf
import numpy as np
import os

# Chemin vers votre modÃ¨le TFLite gÃ©nÃ©rÃ©
# Exemple : './neuroflux_optimized_models_fr/mobilenet/mobilenet_optimized.tflite'
# Assurez-vous que ce chemin correspond Ã  un modÃ¨le que vous avez gÃ©nÃ©rÃ©
model_path = "./neuroflux_optimized_models_fr/mobilenet/mobilenet_optimized.tflite" # Adaptez ce chemin

if not os.path.exists(model_path):
    print(f"ModÃ¨le non trouvÃ© Ã  {model_path}. Veuillez d'abord gÃ©nÃ©rer les modÃ¨les.")
else:
    # Charger le modÃ¨le TFLite et allouer les tenseurs.
    interpreteur = tf.lite.Interpreter(model_path=model_path)
    interpreteur.allocate_tensors()

    # Obtenir les tenseurs d'entrÃ©e et de sortie.
    details_entree = interpreteur.get_input_details()
    details_sortie = interpreteur.get_output_details()

    # PrÃ©parer des donnÃ©es d'entrÃ©e factices
    # Cet exemple suppose une entrÃ©e image 1x224x224x3 pour un modÃ¨le comme MobileNet
    # Ajustez la forme (shape) et le type de donnÃ©es (dtype) selon les exigences de votre modÃ¨le
    forme_entree = details_entree[0]['shape']
    donnees_entree = np.array(np.random.random_sample(forme_entree), dtype=details_entree[0]['dtype'])
    interpreteur.set_tensor(details_entree[0]['index'], donnees_entree)

    # ExÃ©cuter l'infÃ©rence
    interpreteur.invoke()

    # Obtenir les donnÃ©es de sortie
    donnees_sortie = interpreteur.get_tensor(details_sortie[0]['index'])
    print("DonnÃ©es de sortie:", donnees_sortie)
    print("InfÃ©rence TFLite rÃ©ussie !")
```

### ğŸ’¡ Autres Solutions d'InfÃ©rence

Bien que `NeuroFluxDataset` gÃ©nÃ¨re principalement des modÃ¨les TFLite optimisÃ©s, d'autres options existent :
- Pour les modÃ¨les initialement au format ONNX, ou ceux convertibles en ONNX, vous pouvez utiliser **ONNX Runtime Mobile**.
- Pour des dÃ©ploiements encore plus spÃ©cialisÃ©s, **MNN** et **NCNN** sont des moteurs d'infÃ©rence puissants ; vous pouvez explorer leurs outils pour convertir des modÃ¨les Ã  leurs formats respectifs.

### ğŸ—ï¸ Structure du Projet

```
neuroflux/
â”œâ”€â”€ src/                  # Code source
â”‚   â”œâ”€â”€ models/           # Nano-modÃ¨les
â”‚   â”œâ”€â”€ engines/          # Moteurs d'orchestration
â”‚   â””â”€â”€ pheromones/      # SystÃ¨me de phÃ©romones
â”œâ”€â”€ wasm/                 # Compilation WebAssembly
â”œâ”€â”€ docs/                # Documentation
â””â”€â”€ tests/               # Tests
```

### ğŸš€ Premier ModÃ¨le

1. **CrÃ©ez un ModÃ¨le TinyBERT**
   ```python
   from models.tinybert.model import TinyBERT
   
   # CrÃ©er un modÃ¨le
   model = TinyBERT()
   
   # Optimiser
   model.optimize()
   ```

2. **Compilez en WebAssembly**
   ```bash
   python wasm/tinybert_compile.py
   ```

### ğŸ¤– Essaim de PhÃ©romones

1. **Initialisez la Base de DonnÃ©es**
   ```python
   from src.pheromones.digital_pheromones import PheromoneDatabase
   
   # Initialiser la base
   db = PheromoneDatabase()
   ```

2. **CrÃ©ez une PhÃ©romone**
   ```python
   from src.pheromones.digital_pheromones import DigitalPheromone
   
   # CrÃ©er une phÃ©romone
   pheromone = DigitalPheromone(
       agent_id="agent_1",
       signal_type="knowledge",
       data={"topic": "AI", "confidence": 0.9},
       ttl=3600  # 1 heure
   )
   
   # Ajouter Ã  la base
   db.add_pheromone(pheromone)
   ```

### ğŸ§ª Tests

Pour exÃ©cuter les tests :

```bash
pytest tests/ -v
```

### ğŸ“š Documentation

La documentation complÃ¨te est disponible sur GitHub Pages :

[https://neuroflux.github.io/neuroflux/](https://neuroflux.github.io/neuroflux/)

### ğŸ¤ Contribuer

Voir [CONTRIBUTING.md](https://github.com/neuroflux/neuroflux/blob/main/CONTRIBUTING.md) pour savoir comment contribuer.

### ğŸ“ Licence

Ce projet est sous licence Apache 2.0. Voir [LICENSE](https://github.com/neuroflux/neuroflux/blob/main/LICENSE) pour plus de dÃ©tails.

## ğŸŒ In English

## ğŸš€ Quick Start NeuroFlux

### ğŸ“‹ Prerequisites

- Python 3.10 or higher
- Git
- Python virtual environment

### ğŸ“¦ Installation

1. **Clone the Repository**
   ```bash
   git clone https://github.com/neuroflux/neuroflux.git
   cd neuroflux
   ```

2. **Create a Virtual Environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   venv\Scripts\activate     # Windows
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

### ğŸ§  Accessing Pre-Optimized NeuroFlux Models

NeuroFlux provides a suite of AI models pre-optimized for edge deployment.
You can prepare these models using the `NeuroFluxDataset` utility from the `huggingface` module.

First, ensure you have the necessary libraries for model conversion and optimization:
```bash
pip install -r huggingface/requirements.txt
# You might also need to install specific conversion tools like onnx-tensorflow or paddle2onnx
# pip install onnx onnx-tf paddle2onnx # Example
```

Then, you can use the following Python script to generate the optimized models:

```python
from huggingface.neuroflux import NeuroFluxDataset
import os

# Create an instance for all models (or a specific one)
# The 'name' in NeuroFluxConfig will determine which models are processed.
# For this example, let's assume you have a config in neuroflux.py
# that processes a model like 'mobilenet' or 'efficientnet-lite'.

# Example: Process the 'mobilenet' model type
print("Preparing NeuroFlux MobileNet model...")
# Ensure "mobilenet" is a valid config name from NeuroFluxDataset.BUILDER_CONFIGS
dataset_builder = NeuroFluxDataset(name="mobilenet")

# Define an output directory for the dataset
output_dir = "./neuroflux_optimized_models_en" # Language-specific output directory
os.makedirs(output_dir, exist_ok=True)

# This will download, convert, and optimize the models specified by the config
# The resulting TFLite models and metadata will be in 'output_dir'
dataset_builder.download_and_prepare(download_dir=output_dir)

print(f"Optimized models and metadata are available in: {output_dir}")
print("You can find .tflite model files in the respective model subdirectories.")
```

This includes a variety of models (e.g., for vision like EfficientNet-Lite and NanoDet-Plus, NLP models like FastBERT, and more). These models are automatically converted to TFLite and can be optimized for GPU execution and hybrid quantization for enhanced performance on mobile/edge devices.

### ğŸš€ Running Inference with a TFLite Model

Once you have a TFLite model generated (e.g., `mobilenet_optimized.tflite` in the `output_dir/mobilenet/` directory), you can run inference using the TensorFlow Lite Interpreter. Ensure you have `tensorflow` and `numpy` installed (`pip install tensorflow numpy`).

```python
import tensorflow as tf
import numpy as np
import os

# Path to your generated TFLite model
# Example: './neuroflux_optimized_models_en/mobilenet/mobilenet_optimized.tflite'
# Ensure this path matches a model you have generated
model_path = "./neuroflux_optimized_models_en/mobilenet/mobilenet_optimized.tflite" # Adjust this path

if not os.path.exists(model_path):
    print(f"Model not found at {model_path}. Please generate models first.")
else:
    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Prepare dummy input data
    # This example assumes a 1x224x224x3 image input for a model like MobileNet
    # Adjust the shape and dtype according to your model's input requirements
    input_shape = input_details[0]['shape']
    input_data = np.array(np.random.random_sample(input_shape), dtype=input_details[0]['dtype'])
    interpreter.set_tensor(input_details[0]['index'], input_data)

    # Run inference
    interpreter.invoke()

    # Get output data
    output_data = interpreter.get_tensor(output_details[0]['index'])
    print("Output data:", output_data)
    print("TFLite Inference successful!")
```

### ğŸ’¡ Other Inference Solutions

The `NeuroFluxDataset` primarily generates optimized TFLite models. For models originally in ONNX format, or those convertible to ONNX, you can also use **ONNX Runtime Mobile**. For even more specialized deployments, **MNN** and **NCNN** are powerful inference engines; you can explore their tools to convert models to their respective formats.

### ğŸ—ï¸ Project Structure

```
neuroflux/
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ models/           # Nano-models
â”‚   â”œâ”€â”€ engines/          # Orchestration engines
â”‚   â””â”€â”€ pheromones/      # Pheromone system
â”œâ”€â”€ wasm/                 # WebAssembly compilation
â”œâ”€â”€ docs/                # Documentation
â””â”€â”€ tests/               # Tests
```

### ğŸš€ First Model

1. **Create a TinyBERT Model**
   ```python
   from models.tinybert.model import TinyBERT
   
   # Create a model
   model = TinyBERT()
   
   # Optimize
   model.optimize()
   ```

2. **Compile to WebAssembly**
   ```bash
   python wasm/tinybert_compile.py
   ```

### ğŸ¤– Pheromone Swarm

1. **Initialize the Database**
   ```python
   from src.pheromones.digital_pheromones import PheromoneDatabase
   
   # Initialize the database
   db = PheromoneDatabase()
   ```

2. **Create a Pheromone**
   ```python
   from src.pheromones.digital_pheromones import DigitalPheromone
   
   # Create a pheromone
   pheromone = DigitalPheromone(
       agent_id="agent_1",
       signal_type="knowledge",
       data={"topic": "AI", "confidence": 0.9},
       ttl=3600  # 1 hour
   )
   
   # Add to database
   db.add_pheromone(pheromone)
   ```

### ğŸ§ª Tests

To run the tests:

```bash
pytest tests/ -v
```

### ğŸ“š Documentation

The complete documentation is available on GitHub Pages:

[https://neuroflux.github.io/neuroflux/](https://neuroflux.github.io/neuroflux/)

### ğŸ¤ Contributing

See [CONTRIBUTING.md](https://github.com/neuroflux/neuroflux/blob/main/CONTRIBUTING.md) to learn how to contribute.

### ğŸ“ License

This project is under Apache 2.0 license. See [LICENSE](https://github.com/neuroflux/neuroflux/blob/main/LICENSE) for details.
