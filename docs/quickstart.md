# üöÄ D√©marrage Rapide NeuroFlux

## üåç En Fran√ßais

Bienvenue dans le guide de d√©marrage rapide de NeuroFlux ! Suivez ces √©tapes simples pour commencer √† utiliser notre framework d'IA microscopique.

### üìã Pr√©requis

- Python 3.10 ou sup√©rieur
- Git
- Un environnement virtuel Python

### üì¶ Installation

1. **Clonez le Repository**
   ```bash
   git clone https://github.com/neuroflux/neuroflux.git
   cd neuroflux
   ```

2. **Cr√©ez un Environnement Virtuel**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   venv\Scripts\activate     # Windows
   ```

3. **Installez les D√©pendances**
   ```bash
   pip install -r requirements.txt
   ```

### üß† Acc√©der aux Mod√®les NeuroFlux Pr√©-Optimis√©s

NeuroFlux fournit une suite de mod√®les IA pr√©-optimis√©s pour le d√©ploiement sur des appareils p√©riph√©riques (edge).
Vous pouvez pr√©parer ces mod√®les en utilisant l'utilitaire `NeuroFluxDataset` du module `huggingface`.

Assurez-vous d'abord d'avoir les biblioth√®ques n√©cessaires pour la conversion et l'optimisation des mod√®les :
```bash
pip install -r huggingface/requirements.txt
# Vous pourriez √©galement avoir besoin d'installer des outils de conversion sp√©cifiques comme onnx-tensorflow ou paddle2onnx
# pip install onnx onnx-tf paddle2onnx # Exemple
```

Ensuite, vous pouvez utiliser le script Python suivant pour g√©n√©rer les mod√®les optimis√©s :

```python
from huggingface.neuroflux import NeuroFluxDataset
import os

# Cr√©ez une instance pour tous les mod√®les (ou un sp√©cifique)
# Le 'name' dans NeuroFluxConfig d√©terminera quels mod√®les sont trait√©s.
# Pour cet exemple, supposons que vous ayez une configuration dans neuroflux.py
# qui traite un mod√®le comme 'mobilenet' ou 'efficientnet-lite'.

# Exemple : Traiter le type de mod√®le 'mobilenet'
print("Pr√©paration du mod√®le NeuroFlux MobileNet...")
# Assurez-vous que "mobilenet" est un nom de configuration valide dans NeuroFluxDataset.BUILDER_CONFIGS
dataset_builder = NeuroFluxDataset(name="mobilenet")

# D√©finir un r√©pertoire de sortie pour le dataset
output_dir = "./neuroflux_optimized_models_fr" # R√©pertoire de sortie sp√©cifique √† la langue
os.makedirs(output_dir, exist_ok=True)

# Ceci va t√©l√©charger, convertir et optimiser les mod√®les sp√©cifi√©s par la configuration
# Les mod√®les TFLite r√©sultants et les m√©tadonn√©es seront dans 'output_dir'
dataset_builder.download_and_prepare(download_dir=output_dir)

print(f"Les mod√®les optimis√©s et les m√©tadonn√©es sont disponibles dans : {output_dir}")
print("Vous pouvez trouver les fichiers de mod√®le .tflite dans les sous-r√©pertoires respectifs des mod√®les.")
```

Cela inclut une vari√©t√© de mod√®les (par exemple, pour la vision comme EfficientNet-Lite et NanoDet-Plus, des mod√®les NLP comme FastBERT, et plus encore). Ces mod√®les sont automatiquement convertis en TFLite et peuvent √™tre optimis√©s pour l'ex√©cution GPU et la quantification hybride afin d'am√©liorer les performances sur les appareils mobiles/p√©riph√©riques.

### üöÄ Ex√©cuter l'Inf√©rence avec un Mod√®le TFLite

Une fois que vous avez g√©n√©r√© un mod√®le TFLite (par exemple, `mobilenet.tflite` dans le r√©pertoire `output_dir/mobilenet/`), vous pouvez ex√©cuter une inf√©rence en utilisant TensorFlow Lite Interpreter. Assurez-vous d'avoir `tensorflow` et `numpy` install√©s (`pip install tensorflow numpy`).

```python
import tensorflow as tf
import numpy as np
import os

# Chemin vers votre mod√®le TFLite g√©n√©r√©
# Exemple : './neuroflux_optimized_models_fr/mobilenet/mobilenet_optimized.tflite'
# Assurez-vous que ce chemin correspond √† un mod√®le que vous avez g√©n√©r√©
model_path = "./neuroflux_optimized_models_fr/mobilenet/mobilenet_optimized.tflite" # Adaptez ce chemin

if not os.path.exists(model_path):
    print(f"Mod√®le non trouv√© √† {model_path}. Veuillez d'abord g√©n√©rer les mod√®les.")
else:
    # Charger le mod√®le TFLite et allouer les tenseurs.
    interpreteur = tf.lite.Interpreter(model_path=model_path)
    interpreteur.allocate_tensors()

    # Obtenir les tenseurs d'entr√©e et de sortie.
    details_entree = interpreteur.get_input_details()
    details_sortie = interpreteur.get_output_details()

    # Pr√©parer des donn√©es d'entr√©e factices
    # Cet exemple suppose une entr√©e image 1x224x224x3 pour un mod√®le comme MobileNet
    # Ajustez la forme (shape) et le type de donn√©es (dtype) selon les exigences de votre mod√®le
    forme_entree = details_entree[0]['shape']
    donnees_entree = np.array(np.random.random_sample(forme_entree), dtype=details_entree[0]['dtype'])
    interpreteur.set_tensor(details_entree[0]['index'], donnees_entree)

    # Ex√©cuter l'inf√©rence
    interpreteur.invoke()

    # Obtenir les donn√©es de sortie
    donnees_sortie = interpreteur.get_tensor(details_sortie[0]['index'])
    print("Donn√©es de sortie:", donnees_sortie)
    print("Inf√©rence TFLite r√©ussie !")
```

### üí° Autres Solutions d'Inf√©rence

Bien que `NeuroFluxDataset` g√©n√®re principalement des mod√®les TFLite optimis√©s, d'autres options existent :
- Pour les mod√®les initialement au format ONNX, ou ceux convertibles en ONNX, vous pouvez utiliser **ONNX Runtime Mobile**.
- Pour des d√©ploiements encore plus sp√©cialis√©s, **MNN** et **NCNN** sont des moteurs d'inf√©rence puissants ; vous pouvez explorer leurs outils pour convertir des mod√®les √† leurs formats respectifs.

### üèóÔ∏è Structure du Projet

```
neuroflux/
‚îú‚îÄ‚îÄ src/                  # Code source
‚îÇ   ‚îú‚îÄ‚îÄ models/           # Nano-mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ engines/          # Moteurs d'orchestration
‚îÇ   ‚îî‚îÄ‚îÄ pheromones/      # Syst√®me de ph√©romones
‚îú‚îÄ‚îÄ wasm/                 # Compilation WebAssembly
‚îú‚îÄ‚îÄ docs/                # Documentation
‚îî‚îÄ‚îÄ tests/               # Tests
```

### üöÄ Premier Mod√®le

1. **Cr√©ez un Mod√®le TinyBERT**
   ```python
   from models.tinybert.model import TinyBERT
   
   # Cr√©er un mod√®le
   model = TinyBERT()
   
   # Optimiser
   model.optimize()
   ```

2. **Compilez en WebAssembly**
   ```bash
   python wasm/tinybert_compile.py
   ```

### ü§ñ Essaim de Ph√©romones

1. **Initialisez la Base de Donn√©es**
   ```python
   from src.pheromones.digital_pheromones import PheromoneDatabase
   
   # Initialiser la base
   db = PheromoneDatabase()
   ```

2. **Cr√©ez une Ph√©romone**
   ```python
   from src.pheromones.digital_pheromones import DigitalPheromone
   
   # Cr√©er une ph√©romone
   pheromone = DigitalPheromone(
       agent_id="agent_1",
       signal_type="knowledge",
       data={"topic": "AI", "confidence": 0.9},
       ttl=3600  # 1 heure
   )
   
   # Ajouter √† la base
   db.add_pheromone(pheromone)
   ```

### üß™ Tests

Pour ex√©cuter les tests :

```bash
pytest tests/ -v
```

### üìö Documentation

La documentation compl√®te est disponible sur GitHub Pages :

[https://neuroflux.github.io/neuroflux/](https://neuroflux.github.io/neuroflux/)

### ü§ù Contribuer

Voir [CONTRIBUTING.md](https://github.com/neuroflux/neuroflux/blob/main/CONTRIBUTING.md) pour savoir comment contribuer.

### üìù Licence

Ce projet est sous licence Apache 2.0. Voir [LICENSE](https://github.com/neuroflux/neuroflux/blob/main/LICENSE) pour plus de d√©tails.

## üåê In English

## üöÄ Quick Start NeuroFlux

### üìã Prerequisites

- Python 3.10 or higher
- Git
- Python virtual environment

### üì¶ Installation

1. **Clone the Repository**
   ```bash
   git clone https://github.com/neuroflux/neuroflux.git
   cd neuroflux
   ```

2. **Create a Virtual Environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   venv\Scripts\activate     # Windows
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

### üß† Accessing Pre-Optimized NeuroFlux Models

NeuroFlux provides a suite of AI models pre-optimized for edge deployment.
You can prepare these models using the `NeuroFluxDataset` utility from the `huggingface` module.

First, ensure you have the necessary libraries for model conversion and optimization:
```bash
pip install -r huggingface/requirements.txt
# You might also need to install specific conversion tools like onnx-tensorflow or paddle2onnx
# pip install onnx onnx-tf paddle2onnx # Example
```

Then, you can use the following Python script to generate the optimized models:

```python
from huggingface.neuroflux import NeuroFluxDataset
import os

# Create an instance for all models (or a specific one)
# The 'name' in NeuroFluxConfig will determine which models are processed.
# For this example, let's assume you have a config in neuroflux.py
# that processes a model like 'mobilenet' or 'efficientnet-lite'.

# Example: Process the 'mobilenet' model type
print("Preparing NeuroFlux MobileNet model...")
# Ensure "mobilenet" is a valid config name from NeuroFluxDataset.BUILDER_CONFIGS
dataset_builder = NeuroFluxDataset(name="mobilenet")

# Define an output directory for the dataset
output_dir = "./neuroflux_optimized_models_en" # Language-specific output directory
os.makedirs(output_dir, exist_ok=True)

# This will download, convert, and optimize the models specified by the config
# The resulting TFLite models and metadata will be in 'output_dir'
dataset_builder.download_and_prepare(download_dir=output_dir)

print(f"Optimized models and metadata are available in: {output_dir}")
print("You can find .tflite model files in the respective model subdirectories.")
```

This includes a variety of models (e.g., for vision like EfficientNet-Lite and NanoDet-Plus, NLP models like FastBERT, and more). These models are automatically converted to TFLite and can be optimized for GPU execution and hybrid quantization for enhanced performance on mobile/edge devices.

### üöÄ Running Inference with a TFLite Model

Once you have a TFLite model generated (e.g., `mobilenet_optimized.tflite` in the `output_dir/mobilenet/` directory), you can run inference using the TensorFlow Lite Interpreter. Ensure you have `tensorflow` and `numpy` installed (`pip install tensorflow numpy`).

```python
import tensorflow as tf
import numpy as np
import os

# Path to your generated TFLite model
# Example: './neuroflux_optimized_models_en/mobilenet/mobilenet_optimized.tflite'
# Ensure this path matches a model you have generated
model_path = "./neuroflux_optimized_models_en/mobilenet/mobilenet_optimized.tflite" # Adjust this path

if not os.path.exists(model_path):
    print(f"Model not found at {model_path}. Please generate models first.")
else:
    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path=model_path)
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Prepare dummy input data
    # This example assumes a 1x224x224x3 image input for a model like MobileNet
    # Adjust the shape and dtype according to your model's input requirements
    input_shape = input_details[0]['shape']
    input_data = np.array(np.random.random_sample(input_shape), dtype=input_details[0]['dtype'])
    interpreter.set_tensor(input_details[0]['index'], input_data)

    # Run inference
    interpreter.invoke()

    # Get output data
    output_data = interpreter.get_tensor(output_details[0]['index'])
    print("Output data:", output_data)
    print("TFLite Inference successful!")
```

### üí° Other Inference Solutions

The `NeuroFluxDataset` primarily generates optimized TFLite models. For models originally in ONNX format, or those convertible to ONNX, you can also use **ONNX Runtime Mobile**. For even more specialized deployments, **MNN** and **NCNN** are powerful inference engines; you can explore their tools to convert models to their respective formats.

### üèóÔ∏è Project Structure

```
neuroflux/
‚îú‚îÄ‚îÄ src/                  # Source code
‚îÇ   ‚îú‚îÄ‚îÄ models/           # Nano-models
‚îÇ   ‚îú‚îÄ‚îÄ engines/          # Orchestration engines
‚îÇ   ‚îî‚îÄ‚îÄ pheromones/      # Pheromone system
‚îú‚îÄ‚îÄ wasm/                 # WebAssembly compilation
‚îú‚îÄ‚îÄ docs/                # Documentation
‚îî‚îÄ‚îÄ tests/               # Tests
```

### üöÄ First Model

1. **Create a TinyBERT Model**
   ```python
   from models.tinybert.model import TinyBERT
   
   # Create a model
   model = TinyBERT()
   
   # Optimize
   model.optimize()
   ```

2. **Compile to WebAssembly**
   ```bash
   python wasm/tinybert_compile.py
   ```

### ü§ñ Pheromone Swarm

1. **Initialize the Database**
   ```python
   from src.pheromones.digital_pheromones import PheromoneDatabase
   
   # Initialize the database
   db = PheromoneDatabase()
   ```

2. **Create a Pheromone**
   ```python
   from src.pheromones.digital_pheromones import DigitalPheromone
   
   # Create a pheromone
   pheromone = DigitalPheromone(
       agent_id="agent_1",
       signal_type="knowledge",
       data={"topic": "AI", "confidence": 0.9},
       ttl=3600  # 1 hour
   )
   
   # Add to database
   db.add_pheromone(pheromone)
   ```

### üß™ Tests

To run the tests:

```bash
pytest tests/ -v
```

### üìö Documentation

The complete documentation is available on GitHub Pages:

[https://neuroflux.github.io/neuroflux/](https://neuroflux.github.io/neuroflux/)

### ü§ù Contributing

See [CONTRIBUTING.md](https://github.com/neuroflux/neuroflux/blob/main/CONTRIBUTING.md) to learn how to contribute.

### üìù License

This project is under Apache 2.0 license. See [LICENSE](https://github.com/neuroflux/neuroflux/blob/main/LICENSE) for details.
